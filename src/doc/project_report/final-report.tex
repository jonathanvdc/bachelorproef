\documentclass[a4paper,12pt]{article}
\usepackage[backend=bibtex]{biblatex}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{parskip}
\usepackage{microtype}
\usepackage{hyperref}
\usepackage{pxfonts}
\frenchspacing
\bibliography{final-report}

\newcommand{\sourcefile}[1]{\texttt{src/main/cpp/#1}}
\newcommand{\shorttypename}[1]{\texttt{#1}}
% \newcommand{\typename}[2]{\texttt{#1::#2}} % namespaces (qualified names)
\newcommand{\typename}[2]{\texttt{#2}} % no namespaces (unqualified names)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Project Report: Flu++ Group}
\author{Sibert Aerts, C\'edric De Haes,\\ Jonathan Van der Cruysse, Lynn Van Hauwe}
\date{June 2017}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\maketitle
\section*{Preface}
This document is a retrospective report for our bachelor's degree group project: a fork of the \emph{Stride} disease modeling simulator. \autocite{bachelorproef} We will give an overview of the features we added, and describe the hurdles and obstacles we ran into while implementing each of them.

\tableofcontents
\pagebreak

\section{HDF5 checkpointing}
% TODO

\section{Parallelization}

The parallelization sub-task was implemented in full and extended with some extra features. Additional features include: a uniform parallelization API, two parallelization implementations that do not depend on external libraries, and a parallel map implementation that has been integrated into the \typename{stride}{Population} type.

\subsection{Parallelization libraries}

The CMake script will pick an appropriate parallelization library at configure-time and build Stride for that library.

We implemented parallelization by first creating a small common API (\sourcefile{util/Parallel.h}) and then implementing that API for specific parallelization libraries. In total we have created four implementations: OpenMP, TBB, a home-grown implementation based on standard library threads, and a reference implementation that does not distribute work across processing units.

The user manual includes a detailed overview of how the configuration process works, along with a performance comparison of parallelization libraries.

\subsection{Parallelizing maps}

\subsubsection{Approach}

We also tried to parallelize operations on (dense) \typename{std}{map<K, V>} contents. In this context it proved somewhat difficult to carve up the data structure in chunks before processing each chunk separately. \typename{std}{map<K, V>} does not provide an API that allows us to create such chunks nor does it support getting an iterator to the $n$th element like an \typename{std}{vector<T>}.

We resorted to a clever trick: we request the minimum and maximum keys in the \typename{std}{map<K, V>} and partition $\left[\text{min}, \text{max}\right]$ into $n$ ranges $\left[k_i, k_{i+1}\right)$ of (quasi-)equal length. We then request iterators to the key-value pairs with key $k_i$.

If some $k_i$ is not a key in the \typename{std}{map<K, V>}, then we insert a dummy pair $\left(k_i, \shorttypename{V()}\right)$ into the map, get the iterator to that pair, advance said iterator to the next pair, and delete the dummy pair we inserted.

Once we have an iterator starting at key $k'_i$ for each $k_i$ such that $k_i \le k'_i \le k'_{i+1}$ for $i \in \left\{0, 1, \dots, n\right\}$, we spawn a number of threads an give each thread a range of keys $\left[k'_i, k'_{i+1}\right)$ to process.

\subsubsection{Complexity analysis}

A simple complexity analysis shows that our chunking techniques scales well. Let $d\left(m\right)$ be the number of operations it takes for \typename{std}{map<K, V>} to get an iterator to an element, insert an element, increment an iterator, and delete an element. We know that $d \in O\left(\log m\right)$.

We want to start $n$ threads, which implies that we need to obtain $n + 1$ iterators. This yields us an overall time complexity of $O\left(n \cdot \log m\right)$.

The complexity obtained above is quite suitable for quickly dividing a container into chunks---$n$ tends to be rather small ($n \le 16$ even for high-end desktop machines) and $\log m$ is also small, even for large values of $m$.

\subsubsection{Drawbacks}

There are two main drawbacks to the approach we use to carve up \typename{std}{map<K, V>} instances:

\begin{itemize}
	\item We assume that keys are distributed more or less uniformly. This is a fundamental limitation: there's nothing we can do about. Fortunately, our assumption turned out to be mostly correct for all \typename{std}{map<K, V>} values on which we wanted to operate in parallel.
	
	\item Carving the \typename{std}{map<K, V>} into chunks mutates the container's contents. This is non-obvious and may bite users that want to run multiple parallel queries \emph{simultaneously}---even pure parallel queries are thread-unsafe if the approach outlined above is used na\"ively.
\end{itemize}

We mitigated the effects of the latter drawback by creating a new data structure: \typename{stride::util::parallel}{ParallelMap<K, V>} wraps an \typename{std}{map<K, V>} and a reader--writer lock. \typename{stride::util::parallel}{ParallelMap<K, V>} offers the same interface as \typename{std}{map<K, V>} and augments that interface with parallel and serial iteration functionality.

When an operation is to be applied in parallel to the contents of a \typename{stride::util::parallel}{ParallelMap<K, V>}, a (unique) writer lock is held while chunks are created. All other operations, including the operations inherited from \typename{std}{map<K, V>}, acquire a (shared) reader lock. The consequence is that, if used in isolation, serial map functionality will never block, and parallel usage will block only while carving out chunks (which is rather fast).

Note that \typename{stride::util::parallel}{ParallelMap<K, V>} is \emph{not} a thread-safe wrapper around \typename{std}{map<K, V>}. Rather, it offers the \emph{same} thread-safety characteristics, with the addition of thread-safe parallel and serial iteration functionality.

\subsubsection{Performance analysis}

\autoref{fig:parallel-map-performance} presents statistics to evaluate iterating over all elements in a \typename{stride::util::parallel}{ParallelMap<K, V>} serially or in parallel. Over the course of fifty runs per map size, eight threads were applied to \typename{stride::util::parallel}{ParallelMap<K, V>} that contained one hundred, five hundred or one thousand elements. Processing a single element from the array takes approximately one millisecond. The measurements were produced by an Intel Core i7-6700K CPU running Ubuntu 17.04 and gcc 6.3.0.

\begin{figure}[h]
	\begin{tabular}{r|r|r|r|r|r}
		{\footnotesize \textbf{elems}} & {\footnotesize \textbf{serial mean}} & {\footnotesize \textbf{parallel mean}} & {\footnotesize \textbf{serial stddev.}} & {\footnotesize \textbf{parallel stddev.}} & {\footnotesize \textbf{speedup}} \\
		100 & 0.110s & 0.028s & 0.002s & 0.007s & 3.93 \\
		500 & 0.553s & 0.083s & 0.006s & 0.007s & 6.66 \\
		1,000 & 1.111s & 0.158s & 0.015s & 0.010s & 7.03
	\end{tabular}
	\caption{Statistics for evaluating \typename{stride::util::parallel}{ParallelMap<K, V>} performance}
	\label{fig:parallel-map-performance}
\end{figure}

The table above shows that \typename{stride::util::parallel}{ParallelMap<K, V>} is quite effective for its intended purpose: processing map elements in parallel results in a significant speedup. Most importantly, the speedup factor increases with the number of elements in the map, which makes \typename{stride::util::parallel}{ParallelMap<K, V>} quite suitable for the large collections managed by Stride.

Jonathan implemented parallelization.

\section{Synthetic population generation}
The population generation sub-task was fully implemented: if Stride is run with a population model XML file as the \texttt{population\_file}, the parameters in that file are used to generate a population from scratch.

\subsection{Challenges}
The population generator was challenging in an unusual way, compared to the rest of the project: integrating it into the other pieces was simple, but the requirements were trickier to adhere to.

We initially wrote the population generation code following the first version of the specification as closely as possible, but we (and other groups) concluded that it was difficult, if not impossible, to generate populations for which all the requirements it specified held simultaneously. This part of the spec ended up getting rewritten, meaning we had to start over from scratch.

Generating towns that lie geographically between the specified cities was harder than expected. We measure the convex hull spanned by the pre-existing cities, and sample random points inside it. This way, the simulation area for a geo-distribution profile like \texttt{belgium\_population\_major.csv} is roughly Belgium-shaped, but this only works because Belgium is approximately convex to begin with.

\subsection{Implementation details}
Our generator returns a twofold result: the generated \texttt{Population} contains a collection of person data fitting the parameters defined in the model, for the simulator, and also an \texttt{Atlas} object, which contains geographical data about the generated towns and cities, for the Visualization tool to use.

Lynn wrote both the initial and final versions of the generator.

\section{Multi-region simulations}

We implemented single-process, shared-memory multi-region simulations, but didn't have for multi-process/multi-machine simulations.

\subsection{Single-process, shared-memory multi-region simulations}

The process of implementing single-process, shared-memory multi-region simulations consisted of two parts: refactoring the codebase in preparation for multi-region simulations and actually implementing multi-region simulations. The former task proved to be especially challenging as Stride simulations used to rely on (race-conflict--prone) global variables and adding or removing people from a simulation was only supported at simulation set-up time.

Once refactored, the codebase was updated with support for multi-region simulations. This consisted mostly of designing data structures and adding logic to exchange people between simulations.

The user manual gives a good overview of what multi-region simulations can do.

Jonathan implemented multi-region simulations.

\subsection{Notes on MPI-based multi-region simulation}

We had planned to implement MPI-based multi-region simulation6 setup and transfer by serializing the state of a single-region simulation as a checkpoint, sending the checkpoint to another machine or process as a byte stream and then deserializing the checkpoint into a simulation. We imagined that using HDF5-based checkpoints over an ad-hoc format would save us work, reduce code duplication and reduce serialized checkpoint sizes.

Sadly, checkpointing support materialized later than expected---key single-region simulation checkpointing functionality landed in the master branch just after the beta presentation. Even then, the checkpointing API was not yet entirely suitable for usage in MPI-based multi-region simulations.

Our struggles with integrating checkpointing in MPI-based multi-region simulations can be attributed in part to the fact that restoring a checkpoint does a whole lot more than just recreating the state of a single-region simulation. Furthermore, some of our assumptions about HDF5's capabilities were invalidated. For example, we took for granted that HDF5 would be able to produce and consume in-memory byte streams. This turned out not to be the case, further adding to the list of workarounds we would need to implement to deliver a \emph{prerequisite} to MPI-based multi-region simulation support.

Faced with daunting technical difficulties and a tight schedule, we decided to forgo MPI-based multi-region simulations and focus our efforts elsewhere.

\section{Visualization tool}
We went through various different approaches to implementing visualization during the course of our project.

\subsection{Component-based approach}
Our first approach was to simply extend the simulator with a component that would allow it to render directly to a window as it ran. We quickly decided on \textit{Awesomium} as a suitable visualization library. \textit{Awesomium} boasted to be a HTML-based visualization component compatible with both Windows and Linux builds. We made a very small prototype to verify that the library worked, but when we eventually tried to use it to create a visualization window we discovered that its functionality on Linux had been largely overstated. We found it to be completely useless for our purposes, so we removed it from the project.

We tried to find a replacement in \textit{Qt}, which also boasts an option for HTML-based visualization and multi-platform support. However, after many attempts over a few days, we found it to be very difficult to successfully integrate \textit{Qt} in our project.

These setbacks ended up giving us more room to think about our approach, and instead of moving on to a third visualization library we began to consider the potential downsides of this approach entirely: This approach would not easily allow us to locally visualize a simulation running on a remote server, and it would also make it difficult to implement a way to start the visualization in the middle of a simulation.

\subsection{Server-based approach}
With these issues in mind, we considered a different approach: Extending the simulation with an HTTP server component that would act as an interface allowing external applications to request information. Concretely, the external application would be a web page running in a browser.

This approach would resolve the issues with the component-based approach: Visualizing a simulation running on a remote server would only require setting up a VPN connection. As for starting a visualization in the middle of the simulation's run, the fact the simulation and visualization are independent applications automatically requires and also simplifies this feature.

To this end, we started with integrating \textit{POCO} into the project, a library allowing us to create the desired REST API which would be the interface between the simulation and the visualization.

Issues with this approach, however, was that it did not allow a practical way to visualize a simulation after it had closed, and that the demand for a REST API added unnecessary complexity to implementing the communication between the simulation and the visualization.

Due to these factors we decided to remove \textit{POCO} from the project, and led us to our final approach to visualization.

\subsection{File-based approach}
The file-based approach means the visualizer simply outputs all relevant visualization information as a file so that an external tool can visualize it. Concretely, it outputs the data as a \textit{json} file, and the visualization tool is a web page that runs in a browser.

This approach improves upon the previous ones as now the visualization data exists in a more permanent medium. The simulator is no longer required to be running in order to get access the visualization. Visualization of a remote simulation is still possible, although it now requires synchronizing the output file locally.

A remaining hurdle was the desire to get visualization while the simulation is still running, as initially the file was only written at the end of the simulation, which could potentially take a while. The solution to this was to make the simulator periodically append new data to the output file, and giving the visualizer the ability to detect and read these changes in order to keep its view up-to-date.

A further advantage of this approach is that the interface between the simulation and the visualization is a simple \textit{json} format, described in the user manual. This allows for easy adjustments to the file format that don't require complex changes in the implementations of the visualizer or the simulator.

Sibert implemented visualization.

\section{Overarching tasks}
\subsection{Team cooperation} % how did we divide work? etc.
\subsection{Workflow} % talk about issues and GitHub PRs
\subsection{Tests and CI} % Jenkins, Travis, test reports

We initially set up both Jenkins and Travis CI for our project, but later abandoned Jenkins in favor of an all-Travis workflow.

Whenever someone pushes at least one commit to their fork of the master repository, Travis clones the appropriate branch of said fork. It subsequently follows the instructions in our \texttt{.travis.yml} file, which can be summarized as the following steps:

\begin{enumerate}
	\item Download or build external dependencies such as CMake, a C++ compiler, Boost, \dots
	\item Build Stride
	\item Run all tests
	\item Generate a comparison report and push it to \url{https://flu-plus-plus.github.io/gtest-reports/comparison.html}.
\end{enumerate}

The above happens on every push for the following targets:

\begin{tabular}{l|l|l}
	\textbf{Operating system} & \textbf{Compiler} & \textbf{Parallelization library} \\
	Ubuntu Trusty Tahr & GCC 6.0 & OpenMP \\
	Ubuntu Trusty Tahr & Clang 4.0 & TBB \\
	Ubuntu Trusty Tahr & Clang 4.0 & STL \\
	Mac OS X Sierra & AppleClang (XCode 8.3) & STL \\
\end{tabular}

Travis CI gives us a blank virtual machine on every build, except for some cached folders. Explicitly downloading or building dependencies ensures that our forks of Stride don't bit-rot away by depending on the specifics of the machine hosting a Jenkins build. Cached folders allow us to cache, e.g., compiled Boost binaries, which saves us time on the next build and reduces the strain we put on Travis CI's resources.

In addition to building our commits, Travis also makes sure that our pull requests can be merged into the master repository without regressing on any of the tests. If \texttt{git} can merge a pull request's contents merged into the master repository, Travis will clone the master repository, merge in the pull request locally, and run the tests, as per usual.

All of this is integrated nicely in GitHub's pull request workflow: a status badge tells us if a pull request builds and passes all tests. Additionally, we configured GitHub's \emph{protected branches} feature to require that the Travis build always succeed before merging a pull request into the master branch of our master repository. Barring nondeterminism, this configuration ensures that the master branch's tests never fail.

\subsection{Documentation}
\subsection{The Gantt chart}
\subsection{Merging with \texttt{comp}}

\pagebreak
\printbibliography
% This is assuming there'll be more references, but it might be silly to list them if not. We could inline them into the footnotes or something.

\end{document}
